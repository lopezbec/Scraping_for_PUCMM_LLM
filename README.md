# Fire Crawler (github.com/firecrawl/firecrawl)

## What Firecrawl does
- Crawls entire sites (controls for depth, subdomains, path include/exclude) and handles JS/rate limits. 
- Scrapes to multiple formats (Markdown, HTML, raw HTML, links, screenshots, summaries, JSON), with onlyMainContent and tag include/exclude options. 
- Structured extraction via LLMs using a JSON schema or prompt (/extract or JSON format in /scrape). 
- Search + scrape endpoint to fetch full content for query results.
- Async job model & batching for scale.

## Where Firecrawl is better
- Faster to "LLM-ready" content (clean Markdown/JSON out of the box, including PDFs).
- Schema-based data extraction built in (no separate post-processing step needed to shape JSON).
- Flexible crawl controls (subdomains, external links, depth, includePaths) for precise scope without writing spider code.
- One API for crawl, scrape, and search, reducing glue code and custom infra.

## Missing Features (vs. this repo)
- No dataset-level URL or exact-text de-duplication (you'll still need filter.py).
- No enforced single-language keep (e.g., "Spanish only" filter).
- No repo-specific output layout (data/<domain>_<ts>/*) or crawl_summary.json stats file.
- Limited normalization/cleaning pipeline: no Unicode/whitespace fixes, entity/date standardization, or low-content pruning out of the box.
- No near-duplicate (simhash/fuzzy) detection across pages.
- No automatic site-specific boilerplate heuristics beyond basic main-content options.
- No built-in Markdown‚Üíschema postprocessing tailored to your dataset (LLM extraction exists, but you'll still curate schemas/validators).


# Site‚ÄØCrawler

A lightweight, **fire‚Äëand‚Äëforget web‚Äëscraping toolkit** for collecting, summarising, and cleaning complete website snapshots. Ideal for dataset creation, archival work, or downstream NLP pipelines.

---

## 1‚ÄØ.‚ÄØComponents

| File               | Purpose (one‚Äëliner)                                                                                                                                            | Typical command                                                                                                                                     |
| ------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| `scrappy.py`       | Scrapy spider that visits every link on the target domain (HTML, PDF, TXT) and saves **one‚ÄØJSON file per page** under `data/<domain>_<timestamp>/`.            | `scrapy crawl fullsite \`<br>`  -a domain=ogtic.gob.do \`<br>`  -a start_url=https://ogtic.gob.do/`                                                 |
| `realtimestats.py` | Scrapy **extension** providing live counters and, on shutdown, writing `crawl_summary.json` (pages, words, bytes, duration, etc.) into the same output folder. | *No separate invocation* ‚Äì enabled once listed in **`settings.py`**:<br>`EXTENSIONS = {"site_crawler.extensions.realtimestats.RealTimeStats": 500}` |
| `filter.py`        | Post‚Äëprocessing utility: removes duplicate URLs & texts, keeps only the chosen language, outputs a clean **JSONL** file.                                       | `python filter.py data/ogtic.gob.do_YYYYMMDD_HHMMSS \`<br>`  --out ogtic_clean_es.jsonl \`<br>`  --lang es`                                         |

---

## 2‚ÄØ.‚ÄØQuick‚Äëstart

### 2.1‚ÄØ‚ÄØFull crawl (detached)

```bash
# Launch inside a detached screen so it survives SSH logout
screen -S crawl_ogtic -dm bash -c '
  scrapy crawl fullsite \
    -a domain=ogtic.gob.do \
    -a start_url=https://ogtic.gob.do/ \
    -s CLOSESPIDER_PAGECOUNT=0 \
    2>&1 | tee crawl_ogtic_$(date +%Y%m%d_%H%M%S).log
'

echo "attach with:  screen -r crawl_ogtic   (detach: Ctrl‚ÄëA¬†D)"
```

### 2.2‚ÄØ‚ÄØOutput layout

```
data/
‚îî‚îÄ‚îÄ ogtic.gob.do_YYYYMMDD_HHMMSS/
    ‚îú‚îÄ‚îÄ <page1>.json
    ‚îú‚îÄ‚îÄ <page2>.json
    ‚îú‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ crawl_summary.json   # auto‚Äëgenerated by realtimestats.py
```

### 2.3‚ÄØ‚ÄØPost‚Äëprocess & language‚Äëfilter

```bash
python filter.py data/ogtic.gob.do_YYYYMMDD_HHMMSS \
       --out ogtic_clean_es.jsonl \
       --lang es
```

---

## 3‚ÄØ.‚ÄØConfiguration tips

* **Limit scope** ‚Äì¬†set `allowed_domains`, `CLOSESPIDER_PAGECOUNT`, or `DOWNLOAD_DELAY` in `settings.py`.
* **Respect robots.txt** ‚Äì enable `ROBOTSTXT_OBEY = True` unless you have explicit permission.
* **Extension ordering** ‚Äì RealTimeStats should run **after** core Scrapy stats collectors; `priority¬†=¬†500` works well.

---

## 4‚ÄØ.‚ÄØCurrently working on ‚Äì¬†Data¬†Cleaning¬†üßπ

We are expanding `filter.py` into a richer cleaning pipeline. Planned steps:

1. **Remove boilerplate** (headers, footers, navbars)
2. **Normalize whitespace & Unicode**
3. **Strip HTML tags and scripts**
4. **Remove tracking or non‚Äëcontent phrases**
5. **Discard low‚Äëcontent / garbage pages**
6. **Standardise entities** (dates, numbers, etc.)
7. **Merge broken sentences**
8. **Remove redundant metadata**
9. **Filter by content heuristics**
10. **Apply NLP preprocessing** (tokenisation, lemmatisation, etc.) ‚Äì a bit tricky but worthwhile

### Candidate libraries / tools

| Task                           | Suggested library                        |
| ------------------------------ | ---------------------------------------- |
| HTML parsing & tag removal     | **BeautifulSoup** (`bs4`), `trafilatura` |
| Boilerplate & metadata removal | `trafilatura`                            |
| Unicode normalisation          | `unicodedata`, **ftfy**                  |
| Emoji detection/removal        | `emoji`                                  |
| Language ID                    | `langdetect`, **fastText**               |
| Near‚Äëduplicate detection       | **simhash**, `rapidfuzz`                 |
| NLP preprocessing              | **spaCy**, `nltk`                        |
| Date normalisation             | **dateparser**                           |
| Structured filtering           | **pandas**                               |

Contributions and pull requests are welcome ‚Äì see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## 5‚ÄØ.‚ÄØLicense

[MIT](LICENSE) ‚Äì use it, fork it, share improvements.

---

## 6‚ÄØ.‚ÄØAcknowledgements

Built with ‚ù§Ô∏è on top of [Scrapy](https://scrapy.org/).
