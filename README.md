# Fire Crawler (github.com/firecrawl/firecrawl)

## What Firecrawl does
- Crawls entire sites (controls for depth, subdomains, path include/exclude) and handles JS/rate limits. âœ…
- Scrapes to multiple formats (Markdown, HTML, raw HTML, links, screenshots, summaries, JSON), with onlyMainContent and tag include/exclude options. âœ…
- Structured extraction via LLMs using a JSON schema or prompt (/extract or JSON format in /scrape). âœ…
- Search + scrape endpoint to fetch full content for query results.
- Async job model & batching for scale.

## Where Firecrawl is better
- Faster to "LLM-ready" content (clean Markdown/JSON out of the box, including PDFs).
- Schema-based data extraction built in (no separate post-processing step needed to shape JSON).
- Flexible crawl controls (subdomains, external links, depth, includePaths) for precise scope without writing spider code.
- One API for crawl, scrape, and search, reducing glue code and custom infra.

## Missing Features (vs. this repo)
- No dataset-level URL or exact-text de-duplication (you'll still need filter.py).
- No enforced single-language keep (e.g., "Spanish only" filter).
- No repo-specific output layout (data/<domain>_<ts>/*) or crawl_summary.json stats file.
- Limited normalization/cleaning pipeline: no Unicode/whitespace fixes, entity/date standardization, or low-content pruning out of the box.
- No near-duplicate (simhash/fuzzy) detection across pages.
- No automatic site-specific boilerplate heuristics beyond basic main-content options.
- No built-in Markdownâ†’schema postprocessing tailored to your dataset (LLM extraction exists, but you'll still curate schemas/validators).


# Siteâ€¯Crawler

A lightweight, **fireâ€‘andâ€‘forget webâ€‘scraping toolkit** for collecting, summarising, and cleaning complete website snapshots. Ideal for dataset creation, archival work, or downstream NLP pipelines.

---

## 1â€¯.â€¯Components

| File               | Purpose (oneâ€‘liner)                                                                                                                                            | Typical command                                                                                                                                     |
| ------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| `scrappy.py`       | Scrapy spider that visits every link on the target domain (HTML, PDF, TXT) and saves **oneâ€¯JSON file per page** under `data/<domain>_<timestamp>/`.            | `scrapy crawl fullsite \`<br>`  -a domain=ogtic.gob.do \`<br>`  -a start_url=https://ogtic.gob.do/`                                                 |
| `realtimestats.py` | Scrapy **extension** providing live counters and, on shutdown, writing `crawl_summary.json` (pages, words, bytes, duration, etc.) into the same output folder. | *No separate invocation* â€“ enabled once listed in **`settings.py`**:<br>`EXTENSIONS = {"site_crawler.extensions.realtimestats.RealTimeStats": 500}` |
| `filter.py`        | Postâ€‘processing utility: removes duplicate URLs & texts, keeps only the chosen language, outputs a clean **JSONL** file.                                       | `python filter.py data/ogtic.gob.do_YYYYMMDD_HHMMSS \`<br>`  --out ogtic_clean_es.jsonl \`<br>`  --lang es`                                         |

---

## 2â€¯.â€¯Quickâ€‘start

### 2.1â€¯â€¯Full crawl (detached)

```bash
# Launch inside a detached screen so it survives SSH logout
screen -S crawl_ogtic -dm bash -c '
  scrapy crawl fullsite \
    -a domain=ogtic.gob.do \
    -a start_url=https://ogtic.gob.do/ \
    -s CLOSESPIDER_PAGECOUNT=0 \
    2>&1 | tee crawl_ogtic_$(date +%Y%m%d_%H%M%S).log
'

echo "attach with:  screen -r crawl_ogtic   (detach: Ctrlâ€‘AÂ D)"
```

### 2.2â€¯â€¯Output layout

```
data/
â””â”€â”€ ogtic.gob.do_YYYYMMDD_HHMMSS/
    â”œâ”€â”€ <page1>.json
    â”œâ”€â”€ <page2>.json
    â”œâ”€â”€ ...
    â””â”€â”€ crawl_summary.json   # autoâ€‘generated by realtimestats.py
```

### 2.3â€¯â€¯Postâ€‘process & languageâ€‘filter

```bash
python filter.py data/ogtic.gob.do_YYYYMMDD_HHMMSS \
       --out ogtic_clean_es.jsonl \
       --lang es
```

---

## 3â€¯.â€¯Configuration tips

* **Limit scope** â€“Â set `allowed_domains`, `CLOSESPIDER_PAGECOUNT`, or `DOWNLOAD_DELAY` in `settings.py`.
* **Respect robots.txt** â€“ enable `ROBOTSTXT_OBEY = True` unless you have explicit permission.
* **Extension ordering** â€“ RealTimeStats should run **after** core Scrapy stats collectors; `priorityÂ =Â 500` works well.

---

## 4â€¯.â€¯Currently working on â€“Â DataÂ CleaningÂ ğŸ§¹

We are expanding `filter.py` into a richer cleaning pipeline. Planned steps:

1. **Remove boilerplate** (headers, footers, navbars)
2. **Normalize whitespace & Unicode**
3. **Strip HTML tags and scripts**
4. **Remove tracking or nonâ€‘content phrases**
5. **Discard lowâ€‘content / garbage pages**
6. **Standardise entities** (dates, numbers, etc.)
7. **Merge broken sentences**
8. **Remove redundant metadata**
9. **Filter by content heuristics**
10. **Apply NLP preprocessing** (tokenisation, lemmatisation, etc.) â€“ a bit tricky but worthwhile

### Candidate libraries / tools

| Task                           | Suggested library                        |
| ------------------------------ | ---------------------------------------- |
| HTML parsing & tag removal     | **BeautifulSoup** (`bs4`), `trafilatura` |
| Boilerplate & metadata removal | `trafilatura`                            |
| Unicode normalisation          | `unicodedata`, **ftfy**                  |
| Emoji detection/removal        | `emoji`                                  |
| Language ID                    | `langdetect`, **fastText**               |
| Nearâ€‘duplicate detection       | **simhash**, `rapidfuzz`                 |
| NLP preprocessing              | **spaCy**, `nltk`                        |
| Date normalisation             | **dateparser**                           |
| Structured filtering           | **pandas**                               |

Contributions and pull requests are welcome â€“ see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## 5â€¯.â€¯License

[MIT](LICENSE) â€“ use it, fork it, share improvements.

---

## 6â€¯.â€¯Acknowledgements

Built with â¤ï¸ on top of [Scrapy](https://scrapy.org/).
