# Fire Crawler (github.com/firecrawl/firecrawl)

## What Firecrawl does
- Crawls entire sites (controls for depth, subdomains, path include/exclude) and handles JS/rate limits. ✅
- Scrapes to multiple formats (Markdown, HTML, raw HTML, links, screenshots, summaries, JSON), with onlyMainContent and tag include/exclude options. ✅
- Structured extraction via LLMs using a JSON schema or prompt (/extract or JSON format in /scrape). ✅
- Search + scrape endpoint to fetch full content for query results.
- Async job model & batching for scale.

## Where Firecrawl is better
- Faster to "LLM-ready" content (clean Markdown/JSON out of the box, including PDFs).
- Schema-based data extraction built in (no separate post-processing step needed to shape JSON).
- Flexible crawl controls (subdomains, external links, depth, includePaths) for precise scope without writing spider code.
- One API for crawl, scrape, and search, reducing glue code and custom infra.

## Missing Features (vs. this repo)
- No dataset-level URL or exact-text de-duplication (you'll still need filter.py).
- No enforced single-language keep (e.g., "Spanish only" filter).
- No repo-specific output layout (data/<domain>_<ts>/*) or crawl_summary.json stats file.
- Limited normalization/cleaning pipeline: no Unicode/whitespace fixes, entity/date standardization, or low-content pruning out of the box.
- No near-duplicate (simhash/fuzzy) detection across pages.
- No automatic site-specific boilerplate heuristics beyond basic main-content options.
- No built-in Markdown→schema postprocessing tailored to your dataset (LLM extraction exists, but you'll still curate schemas/validators).


# Site Crawler

A lightweight, **fire‑and‑forget web‑scraping toolkit** for collecting, summarising, and cleaning complete website snapshots. Ideal for dataset creation, archival work, or downstream NLP pipelines.

---

## 1 . Components

| File               | Purpose (one‑liner)                                                                                                                                            | Typical command                                                                                                                                     |
| ------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| `scrappy.py`       | Scrapy spider that visits every link on the target domain (HTML, PDF, TXT) and saves **one JSON file per page** under `data/<domain>_<timestamp>/`.            | `scrapy crawl fullsite \`<br>`  -a domain=ogtic.gob.do \`<br>`  -a start_url=https://ogtic.gob.do/`                                                 |
| `realtimestats.py` | Scrapy **extension** providing live counters and, on shutdown, writing `crawl_summary.json` (pages, words, bytes, duration, etc.) into the same output folder. | *No separate invocation* – enabled once listed in **`settings.py`**:<br>`EXTENSIONS = {"site_crawler.extensions.realtimestats.RealTimeStats": 500}` |
| `filter.py`        | Post‑processing utility: removes duplicate URLs & texts, keeps only the chosen language, outputs a clean **JSONL** file.                                       | `python filter.py data/ogtic.gob.do_YYYYMMDD_HHMMSS \`<br>`  --out ogtic_clean_es.jsonl \`<br>`  --lang es`                                         |

---

## 2 . Quick‑start

### 2.1  Full crawl (detached)

```bash
# Launch inside a detached screen so it survives SSH logout
screen -S crawl_ogtic -dm bash -c '
  scrapy crawl fullsite \
    -a domain=ogtic.gob.do \
    -a start_url=https://ogtic.gob.do/ \
    -s CLOSESPIDER_PAGECOUNT=0 \
    2>&1 | tee crawl_ogtic_$(date +%Y%m%d_%H%M%S).log
'

echo "attach with:  screen -r crawl_ogtic   (detach: Ctrl‑A D)"
```

### 2.2  Output layout

```
data/
└── ogtic.gob.do_YYYYMMDD_HHMMSS/
    ├── <page1>.json
    ├── <page2>.json
    ├── ...
    └── crawl_summary.json   # auto‑generated by realtimestats.py
```

### 2.3  Post‑process & language‑filter

```bash
python filter.py data/ogtic.gob.do_YYYYMMDD_HHMMSS \
       --out ogtic_clean_es.jsonl \
       --lang es
```

---

## 3 . Configuration tips

* **Limit scope** – set `allowed_domains`, `CLOSESPIDER_PAGECOUNT`, or `DOWNLOAD_DELAY` in `settings.py`.
* **Respect robots.txt** – enable `ROBOTSTXT_OBEY = True` unless you have explicit permission.
* **Extension ordering** – RealTimeStats should run **after** core Scrapy stats collectors; `priority = 500` works well.

---

## 4 . Currently working on – Data Cleaning 🧹

We are expanding `filter.py` into a richer cleaning pipeline. Planned steps:

1. **Remove boilerplate** (headers, footers, navbars)
2. **Normalize whitespace & Unicode**
3. **Strip HTML tags and scripts**
4. **Remove tracking or non‑content phrases**
5. **Discard low‑content / garbage pages**
6. **Standardise entities** (dates, numbers, etc.)
7. **Merge broken sentences**
8. **Remove redundant metadata**
9. **Filter by content heuristics**
10. **Apply NLP preprocessing** (tokenisation, lemmatisation, etc.) – a bit tricky but worthwhile

### Candidate libraries / tools

| Task                           | Suggested library                        |
| ------------------------------ | ---------------------------------------- |
| HTML parsing & tag removal     | **BeautifulSoup** (`bs4`), `trafilatura` |
| Boilerplate & metadata removal | `trafilatura`                            |
| Unicode normalisation          | `unicodedata`, **ftfy**                  |
| Emoji detection/removal        | `emoji`                                  |
| Language ID                    | `langdetect`, **fastText**               |
| Near‑duplicate detection       | **simhash**, `rapidfuzz`                 |
| NLP preprocessing              | **spaCy**, `nltk`                        |
| Date normalisation             | **dateparser**                           |
| Structured filtering           | **pandas**                               |

Contributions and pull requests are welcome – see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## 5 . License

[MIT](LICENSE) – use it, fork it, share improvements.

---

## 6 . Acknowledgements

Built with ❤️ on top of [Scrapy](https://scrapy.org/).
